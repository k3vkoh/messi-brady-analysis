{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16f2c02a-68af-4b25-85e8-b3a7f386f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ad216a1-fc97-4395-bd8d-eb82e3734a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load messi data into single string\n",
    "messi_file_path = 'data/messi.json'\n",
    "\n",
    "with open(messi_file_path, 'r') as file:\n",
    "    messidata = json.load(file)\n",
    "\n",
    "messitotalstring = ' '.join(messidata)\n",
    "\n",
    "soup = BeautifulSoup(messitotalstring, 'html.parser')\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "\n",
    "messistring=''\n",
    "for tag in p_tags:\n",
    "    messistring+=tag.get_text()+' '\n",
    "\n",
    "messistring = re.sub(\"\\n\", \" \", messistring)\n",
    "messistring = re.sub(\"\\s+\", \" \", messistring)\n",
    "messistring = re.sub(\"\\[.*?\\]\", \" \", messistring)\n",
    "messistring = messistring.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d981e001-a6fc-46b8-bd13-633f9c839ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trout data into single string\n",
    "trout_file_path = 'data/trout.json'\n",
    "\n",
    "with open(trout_file_path, 'r') as file:\n",
    "    troutdata = json.load(file)\n",
    "\n",
    "trouttotalstring = ' '.join(troutdata)\n",
    "\n",
    "soup = BeautifulSoup(trouttotalstring, 'html.parser')\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "\n",
    "troutstring=''\n",
    "for tag in p_tags:\n",
    "    troutstring+=tag.get_text()+' '\n",
    "\n",
    "troutstring = re.sub(\"\\n\", \" \", troutstring)\n",
    "troutstring = re.sub(\"\\s+\", \" \", troutstring)\n",
    "troutstring = re.sub(\"\\[.*?\\]\", \" \", troutstring)\n",
    "troutstring = troutstring.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5abcde25-8b84-4618-9f9b-76f109846ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lebron data into single string\n",
    "lebron_file_path = 'data/lebron.json'\n",
    "\n",
    "with open(lebron_file_path, 'r') as file:\n",
    "    lebrondata = json.load(file)\n",
    "\n",
    "lebrontotalstring = ' '.join(lebrondata)\n",
    "\n",
    "soup = BeautifulSoup(lebrontotalstring, 'html.parser')\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "\n",
    "lebronstring=''\n",
    "for tag in p_tags:\n",
    "    lebronstring+=tag.get_text()+' '\n",
    "\n",
    "lebronstring = re.sub(\"\\n\", \" \", lebronstring)\n",
    "lebronstring = re.sub(\"\\s+\", \" \", lebronstring)\n",
    "lebronstring = re.sub(\"\\[.*?\\]\", \" \", lebronstring)\n",
    "lebronstring = lebronstring.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6eec73be-6189-4ef5-87a8-675e9f18bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mahomes data into single string\n",
    "mahomes_file_path = 'data/mahomes.json'\n",
    "\n",
    "with open(mahomes_file_path, 'r') as file:\n",
    "    mahomesdata = json.load(file)\n",
    "\n",
    "mahomestotalstring = ' '.join(mahomesdata)\n",
    "\n",
    "soup = BeautifulSoup(mahomestotalstring, 'html.parser')\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "\n",
    "mahomesstring=''\n",
    "for tag in p_tags:\n",
    "    mahomesstring+=tag.get_text()+' '\n",
    "\n",
    "mahomesstring = re.sub(\"\\n\", \" \", mahomesstring)\n",
    "mahomesstring = re.sub(\"\\s+\", \" \", mahomesstring)\n",
    "mahomesstring = re.sub(\"\\[.*?\\]\", \" \", mahomesstring)\n",
    "mahomesstring = mahomesstring.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63fe7557-2f6b-43a4-8d6f-ed3d2fde1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load crosby data into single string\n",
    "crosby_file_path = 'data/crosby.json'\n",
    "\n",
    "with open(crosby_file_path, 'r') as file:\n",
    "    crosbydata = json.load(file)\n",
    "\n",
    "crosbytotalstring = ' '.join(crosbydata)\n",
    "\n",
    "soup = BeautifulSoup(crosbytotalstring, 'html.parser')\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "\n",
    "crosbystring=''\n",
    "for tag in p_tags:\n",
    "    crosbystring+=tag.get_text()+' '\n",
    "\n",
    "crosbystring = re.sub(\"\\n\", \" \", crosbystring)\n",
    "crosbystring = re.sub(\"\\s+\", \" \", crosbystring)\n",
    "crosbystring = re.sub(\"\\[.*?\\]\", \" \", crosbystring)\n",
    "crosbystring = crosbystring.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60e0abe8-adc4-4627-a1f9-118c936ed8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # getting parts of speech from sentences\n",
    "# datas = [messistring, troutstring, lebronstring, mahomesstring, crosbystring]\n",
    "\n",
    "# parts_of_speech = []\n",
    "\n",
    "# for text in datas: \n",
    "#     temp = sent_tokenize(text)\n",
    "#     for x in temp:\n",
    "#         parts_of_speech += pos_tag(word_tokenize(x))\n",
    "\n",
    "# # turn the list of parts of speech to a dictionary\n",
    "# parts_of_speech = list(set(parts_of_speech))\n",
    "# pos_dict = {key: value for key, value in parts_of_speech}\n",
    "\n",
    "# # converting the parts of speech into a form that can be used by the lemmatizer\n",
    "# for key, value in pos_dict.items():\n",
    "#     if value.startswith('J'):\n",
    "#         pos_dict[key] = 'a'\n",
    "#     elif value.startswith('V'):\n",
    "#         pos_dict[key] = 'v'\n",
    "#     elif value.startswith('N'):\n",
    "#         pos_dict[key] = 'n'\n",
    "#     elif value.startswith('R'):\n",
    "#         pos_dict[key] = 'r'\n",
    "#     else:\n",
    "#         pos_dict[key] = 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8aaea6f9-2733-44ca-9ac8-d10421e741ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "messitokens = nltk.word_tokenize(messistring)\n",
    "lebrontokens = nltk.word_tokenize(lebronstring)\n",
    "trouttokens = nltk.word_tokenize(troutstring)\n",
    "mahomestokens = nltk.word_tokenize(mahomesstring)\n",
    "crosbytokens = nltk.word_tokenize(crosbystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0fdd69a-901f-4e02-8bd2-cfbb917edd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Messi Tokens: 233613\n",
      "Length of Lebron Tokens: 60329\n",
      "Length of Trout Tokens: 94955\n",
      "Length of Mahomes Tokens: 120312\n",
      "Length of Crosby Tokens: 23903\n",
      "Length of All Tokens: 533112\n"
     ]
    }
   ],
   "source": [
    "print('Length of Messi Tokens:', len(messitokens))\n",
    "print('Length of Lebron Tokens:', len(lebrontokens))\n",
    "print('Length of Trout Tokens:', len(trouttokens))\n",
    "print('Length of Mahomes Tokens:', len(mahomestokens))\n",
    "print('Length of Crosby Tokens:', len(crosbytokens))\n",
    "\n",
    "total = len(messitokens)+len(lebrontokens)+len(trouttokens)+len(mahomestokens)+len(crosbytokens)\n",
    "print('Length of All Tokens:', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2169e81a-4620-462e-b359-e6fb1aaef20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling list of stopwords\n",
    "\n",
    "punctuation_stop_list = string.punctuation\n",
    "\n",
    "# originial list of stop words\n",
    "stoplist = stopwords.words(\"english\")\n",
    "\n",
    "# adding more stop words\n",
    "stoplist.extend([\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\", \n",
    "    \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \n",
    "    \"can't\", \"cannot\", \"could\", \"couldn't\", \n",
    "    \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\", \"during\", \n",
    "    \"each\", \n",
    "    \"few\", \"for\", \"from\", \"further\", \n",
    "    \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \n",
    "    \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \n",
    "    \"let's\", \n",
    "    \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \n",
    "    \"no\", \"nor\", \"not\", \n",
    "    \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "    \".\", \",\", \"'s\", \"--\", \"n't\", \"ha\", \"wa\"\n",
    "])\n",
    "\n",
    "# adding punctuation\n",
    "stoplist.extend(punctuation_stop_list)\n",
    "\n",
    "# adding more punctuation\n",
    "stoplist.extend([\"''\", '``'])\n",
    "\n",
    "stopwords = set(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43245ae7-3a33-45d9-a829-ec67f3d7425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.2-cp310-cp310-macosx_10_9_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp310-cp310-macosx_10_9_x86_64.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (23.2)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-macosx_10_9_x86_64.whl (26 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (1.26.1)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp310-cp310-macosx_10_9_x86_64.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.2/132.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.10.0,>=0.3.0\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8\n",
      "  Downloading thinc-8.2.1-cp310-cp310-macosx_10_9_x86_64.whl (868 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.7/868.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (65.5.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp310-cp310-macosx_10_9_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (4.66.1)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.14.5\n",
      "  Downloading pydantic_core-2.14.5-cp310-cp310-macosx_10_7_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp310-cp310-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, langcodes, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.5.2 pydantic-core-2.14.5 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: jinja2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4aa274da-cea4-4711-ac4a-329f30c71216",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1192219 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adjectives\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Extract adjectives from each text\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m adjectives_per_text \u001b[38;5;241m=\u001b[39m [extract_adjectives(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# If you want to see the adjectives for each athlete\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(index)):\n",
      "Cell \u001b[0;32mIn[36], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adjectives\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Extract adjectives from each text\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m adjectives_per_text \u001b[38;5;241m=\u001b[39m [\u001b[43mextract_adjectives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# If you want to see the adjectives for each athlete\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(index)):\n",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m, in \u001b[0;36mextract_adjectives\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_adjectives\u001b[39m(text):\n\u001b[0;32m---> 13\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     adjectives \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mpos_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADJ\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adjectives\n",
      "File \u001b[0;32m~/Desktop/textproc/venv/lib/python3.10/site-packages/spacy/language.py:1037\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1018\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Desktop/textproc/venv/lib/python3.10/site-packages/spacy/language.py:1128\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m~/Desktop/textproc/venv/lib/python3.10/site-packages/spacy/language.py:1117\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03mtext (str): The text to process.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m-> 1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1118\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 1192219 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "index=['Messi', 'Lebron', 'Trout', 'Crosby', 'Mahomes']\n",
    "\n",
    "# Your texts\n",
    "texts = [\" \".join(messitokens), \" \".join(lebrontokens), \" \".join(trouttokens), \" \".join(crosbytokens), \" \".join(mahomestokens)]\n",
    "\n",
    "# Function to extract adjectives from a text\n",
    "def extract_adjectives(text):\n",
    "    doc = nlp(text)\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    return adjectives\n",
    "\n",
    "# Extract adjectives from each text\n",
    "adjectives_per_text = [extract_adjectives(text) for text in texts]\n",
    "\n",
    "# If you want to see the adjectives for each athlete\n",
    "for x in range(len(index)):\n",
    "    print(index[x])\n",
    "    print(adjectives_per_text[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4f6d7-6332-4180-8314-ecb4e2811c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# messi_lemmatized = [lemmatizer.lemmatize(w, pos_dict[w]) for w in messitokens]\n",
    "# lebron_lemmatized = [lemmatizer.lemmatize(w, pos_dict[w]) for w in lebrontokens]\n",
    "# mahomes_lemmatized = [lemmatizer.lemmatize(w, pos_dict[w]) for w in mahomestokens]\n",
    "# crosby_lemmatized = [lemmatizer.lemmatize(w, pos_dict[w]) for w in crosbytokens]\n",
    "# trout_lemmatized = [lemmatizer.lemmatize(w, pos_dict[w]) for w in trouttokens]\n",
    "\n",
    "messi_lemmatized = [lemmatizer.lemmatize(w) for w in messitokens]\n",
    "lebron_lemmatized = [lemmatizer.lemmatize(w) for w in lebrontokens]\n",
    "mahomes_lemmatized = [lemmatizer.lemmatize(w) for w in mahomestokens]\n",
    "crosby_lemmatized = [lemmatizer.lemmatize(w) for w in crosbytokens]\n",
    "trout_lemmatized = [lemmatizer.lemmatize(w) for w in trouttokens]\n",
    "\n",
    "messi_nostopwords = [w for w in messi_lemmatized if w not in stopwords]\n",
    "lebron_nostopwords = [w for w in lebron_lemmatized if w not in stopwords]\n",
    "mahomes_nostopwords = [w for w in mahomes_lemmatized if w not in stopwords]\n",
    "crosby_nostopwords = [w for w in crosby_lemmatized if w not in stopwords]\n",
    "trout_nostopwords = [w for w in trout_lemmatized if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ed7fb-68b5-49e1-b5f1-5db9bcae9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building word clouds w stopwords\n",
    "\n",
    "print('Messi Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(messi_lemmatized)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "messitext = \" \".join(messi_lemmatized)                                                                                                                                                                \n",
    "wordcloud = WordCloud(max_font_size=40).generate(messitext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/messi.png')\n",
    "plt.show()\n",
    "\n",
    "print('Lebron Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(lebron_lemmatized)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "lebrontext = \" \".join(lebron_lemmatized)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(lebrontext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/lebron.png')\n",
    "plt.show()\n",
    "\n",
    "print('Mahomes Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(mahomes_lemmatized)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "mahomestext = \" \".join(mahomes_lemmatized)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(mahomestext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/mahomes.png')\n",
    "plt.show()\n",
    "\n",
    "print('Crosby Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(crosby_lemmatized)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "crosbytext = \" \".join(crosby_lemmatized)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(crosbytext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/crosby.png')\n",
    "plt.show()\n",
    "\n",
    "print('Trout Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(trout_lemmatized)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "trouttext = \" \".join(trout_lemmatized)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(trouttext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/trout.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c500a8-640e-4fe4-9ea9-fcfe702b1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building word clouds w/o stopwords\n",
    "\n",
    "print('Messi Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(messi_nostopwords)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "messitext = \" \".join(messi_nostopwords)                                                                                                                                                                \n",
    "wordcloud = WordCloud(max_font_size=40).generate(messitext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/messi.png')\n",
    "plt.show()\n",
    "\n",
    "print('Lebron Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(lebron_nostopwords)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "lebrontext = \" \".join(lebron_nostopwords)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(lebrontext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/lebron.png')\n",
    "plt.show()\n",
    "\n",
    "print('Mahomes Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(mahomes_nostopwords)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "mahomestext = \" \".join(mahomes_nostopwords)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(mahomestext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/mahomes.png')\n",
    "plt.show()\n",
    "\n",
    "print('Crosby Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(crosby_nostopwords)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "crosbytext = \" \".join(crosby_nostopwords)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(crosbytext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/crosby.png')\n",
    "plt.show()\n",
    "\n",
    "print('Trout Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(trout_nostopwords)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "trouttext = \" \".join(trout_nostopwords)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(trouttext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/trout.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268baf30-a9b5-4dcd-a8f3-bf7ff767591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_common_bigrams(tokenlist):\n",
    "    \n",
    "    bigrams = nltk.ngrams(tokenlist, 2)\n",
    "    bigramlist = list(bigrams)\n",
    "    \n",
    "    # print out most frequent bigrams\n",
    "    bigramfreq = nltk.FreqDist(bigramlist)\n",
    "    top10bigrams = bigramfreq.most_common(10)\n",
    "    top50bigrams = bigramfreq.most_common(50)\n",
    "\n",
    "    # part a)\n",
    "    print('** Most frequent bigrams **')\n",
    "    for x in top10bigrams:\n",
    "        print(x[0][0], x[0][1])\n",
    "\n",
    "    \n",
    "    # part b)\n",
    "    print('\\n** Most frequent bigrams with no stop words **')\n",
    "    for x in top50bigrams:\n",
    "        if x[0][0].lower() not in stoplist and x[0][1].lower() not in stoplist:\n",
    "            print(x[0][0], x[0][1])\n",
    "\n",
    "\n",
    "print('Messi Bigrams')\n",
    "print_common_bigrams(messi_lemmatized)\n",
    "print('\\nLebron Bigrams')\n",
    "print_common_bigrams(lebron_lemmatized)\n",
    "print('\\nTrout Bigrams')\n",
    "print_common_bigrams(trout_lemmatized)\n",
    "print('\\nCrosby Bigrams')\n",
    "print_common_bigrams(crosby_lemmatized)\n",
    "print('\\nMahomes Bigrams')\n",
    "print_common_bigrams(mahomes_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f535f-121b-455a-895f-4d35f4386515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statement\n",
    "from nltk.collocations import *\n",
    "\n",
    "# Create the object you need to get collocations.\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "def print_collocations(tokenlist):\n",
    "\n",
    "    finder = BigramCollocationFinder.from_words(tokenlist)\n",
    "    finder.apply_freq_filter(2)\n",
    "    print('** Common Collocations **')\n",
    "    for c in finder.nbest(bigram_measures.pmi, 10):\n",
    "        print(\" \".join(c))\n",
    "\n",
    "print('Messi Collocations')\n",
    "print_collocations(messi_lemmatized)\n",
    "print('\\nLebron Collocations')\n",
    "print_collocations(lebron_lemmatized)\n",
    "print('\\nTrout Collocations')\n",
    "print_collocations(trout_lemmatized)\n",
    "print('\\nCrosby Collocations')\n",
    "print_collocations(crosby_lemmatized)\n",
    "print('\\nMahomes Collocations')\n",
    "print_collocations(mahomes_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933e215-63d6-4d95-ab17-f6c6e20e03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "index=['Messi', 'Lebron', 'Trout', 'Crosby', 'Mahomes']\n",
    "alltexts = [\" \".join(messi_nostopwords), \" \".join(lebron_nostopwords), \" \".join(trout_nostopwords), \" \".join(crosby_nostopwords), \" \".join(mahomes_nostopwords),]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(input='content', stop_words='english')\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(alltexts)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=index, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df = tfidf_df.stack().reset_index()\n",
    "tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document', 'level_1': 'term'})\n",
    "top_terms = tfidf_df.sort_values(by=['document', 'tfidf'], ascending=[True, False]).groupby(['document']).head(10)\n",
    "\n",
    "top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f2371-4a31-40aa-976a-cf8850ead250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will create the heatmap \n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# adding a little randomness to break ties in term ranking\n",
    "top_tfidf_plusRand = top_terms.copy()\n",
    "top_tfidf_plusRand['tfidf'] = top_tfidf_plusRand['tfidf'] + np.random.rand(top_terms.shape[0])*0.0001\n",
    "\n",
    "# base for all visualizations, with rank calculation\n",
    "base = alt.Chart(top_tfidf_plusRand).encode(\n",
    "    x = 'rank:O',\n",
    "    y = 'document:N'\n",
    ").transform_window(\n",
    "    rank = \"rank()\",\n",
    "    sort = [alt.SortField(\"tfidf\", order=\"descending\")],\n",
    "    groupby = [\"document\"],\n",
    ")\n",
    "\n",
    "# heatmap specification\n",
    "heatmap = base.mark_rect().encode(\n",
    "    color = 'tfidf:Q'\n",
    ")\n",
    "\n",
    "# text labels, white for darker heatmap colors\n",
    "text = base.mark_text(baseline='middle').encode(\n",
    "    text = 'term:N',\n",
    "    color = alt.condition(alt.datum.tfidf >= 0.23, alt.value('white'), alt.value('black'))\n",
    ")\n",
    "\n",
    "# display the three superimposed visualizations\n",
    "(heatmap + text).properties(width = 600)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

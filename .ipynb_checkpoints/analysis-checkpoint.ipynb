{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f2c02a-68af-4b25-85e8-b3a7f386f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ad216a1-fc97-4395-bd8d-eb82e3734a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load messi data into single string\n",
    "messi_file_path = 'data/messi.json'\n",
    "\n",
    "with open(messi_file_path, 'r') as file:\n",
    "    messidata = json.load(file)\n",
    "\n",
    "messitotalstring = ' '.join(messidata)\n",
    "\n",
    "soup = BeautifulSoup(messitotalstring, 'html.parser')\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "\n",
    "messistring=''\n",
    "for tag in p_tags:\n",
    "    messistring+=tag.get_text()+' '\n",
    "\n",
    "messistring = re.sub(\"\\n\", \" \", messistring)\n",
    "messistring = re.sub(\"\\s+\", \" \", messistring)\n",
    "messistring = re.sub(\"\\[.*?\\]\", \" \", messistring)\n",
    "messistring = messistring.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d981e001-a6fc-46b8-bd13-633f9c839ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trout data into single string\n",
    "trout_file_path = 'data/trout.json'\n",
    "\n",
    "with open(trout_file_path, 'r') as file:\n",
    "    troutdata = json.load(file)\n",
    "\n",
    "trouttotalstring = ' '.join(troutdata)\n",
    "\n",
    "soup = BeautifulSoup(trouttotalstring, 'html.parser')\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "\n",
    "troutstring=''\n",
    "for tag in p_tags:\n",
    "    troutstring+=tag.get_text()+' '\n",
    "\n",
    "troutstring = re.sub(\"\\n\", \" \", troutstring)\n",
    "troutstring = re.sub(\"\\s+\", \" \", troutstring)\n",
    "troutstring = re.sub(\"\\[.*?\\]\", \" \", troutstring)\n",
    "troutstring = troutstring.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5abcde25-8b84-4618-9f9b-76f109846ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lebron data into single string\n",
    "lebron_file_path = 'data/lebron.json'\n",
    "\n",
    "with open(lebron_file_path, 'r') as file:\n",
    "    lebrondata = json.load(file)\n",
    "\n",
    "lebrontotalstring = ' '.join(lebrondata)\n",
    "\n",
    "soup = BeautifulSoup(lebrontotalstring, 'html.parser')\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "\n",
    "lebronstring=''\n",
    "for tag in p_tags:\n",
    "    lebronstring+=tag.get_text()+' '\n",
    "\n",
    "lebronstring = re.sub(\"\\n\", \" \", lebronstring)\n",
    "lebronstring = re.sub(\"\\s+\", \" \", lebronstring)\n",
    "lebronstring = re.sub(\"\\[.*?\\]\", \" \", lebronstring)\n",
    "lebronstring = lebronstring.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eec73be-6189-4ef5-87a8-675e9f18bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mahomes data into single string\n",
    "mahomes_file_path = 'data/mahomes.json'\n",
    "\n",
    "with open(mahomes_file_path, 'r') as file:\n",
    "    mahomesdata = json.load(file)\n",
    "\n",
    "mahomestotalstring = ' '.join(mahomesdata)\n",
    "\n",
    "soup = BeautifulSoup(mahomestotalstring, 'html.parser')\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "\n",
    "mahomesstring=''\n",
    "for tag in p_tags:\n",
    "    mahomesstring+=tag.get_text()+' '\n",
    "\n",
    "mahomesstring = re.sub(\"\\n\", \" \", mahomesstring)\n",
    "mahomesstring = re.sub(\"\\s+\", \" \", mahomesstring)\n",
    "mahomesstring = re.sub(\"\\[.*?\\]\", \" \", mahomesstring)\n",
    "mahomesstring = mahomesstring.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fe7557-2f6b-43a4-8d6f-ed3d2fde1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load crosby data into single string\n",
    "crosby_file_path = 'data/crosby.json'\n",
    "\n",
    "with open(crosby_file_path, 'r') as file:\n",
    "    crosbydata = json.load(file)\n",
    "\n",
    "crosbytotalstring = ' '.join(crosbydata)\n",
    "\n",
    "soup = BeautifulSoup(crosbytotalstring, 'html.parser')\n",
    "\n",
    "p_tags = soup.find_all('p')\n",
    "\n",
    "crosbystring=''\n",
    "for tag in p_tags:\n",
    "    crosbystring+=tag.get_text()+' '\n",
    "\n",
    "crosbystring = re.sub(\"\\n\", \" \", crosbystring)\n",
    "crosbystring = re.sub(\"\\s+\", \" \", crosbystring)\n",
    "crosbystring = re.sub(\"\\[.*?\\]\", \" \", crosbystring)\n",
    "crosbystring = crosbystring.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aaea6f9-2733-44ca-9ac8-d10421e741ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "messitokens = nltk.word_tokenize(messistring)\n",
    "lebrontokens = nltk.word_tokenize(lebronstring)\n",
    "trouttokens = nltk.word_tokenize(troutstring)\n",
    "mahomestokens = nltk.word_tokenize(mahomesstring)\n",
    "crosbytokens = nltk.word_tokenize(crosbystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0fdd69a-901f-4e02-8bd2-cfbb917edd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Messi Tokens: 233613\n",
      "Length of Lebron Tokens: 60329\n",
      "Length of Trout Tokens: 94955\n",
      "Length of Mahomes Tokens: 120312\n",
      "Length of Crosby Tokens: 23903\n",
      "Length of All Tokens: 533112\n"
     ]
    }
   ],
   "source": [
    "print('Length of Messi Tokens:', len(messitokens))\n",
    "print('Length of Lebron Tokens:', len(lebrontokens))\n",
    "print('Length of Trout Tokens:', len(trouttokens))\n",
    "print('Length of Mahomes Tokens:', len(mahomestokens))\n",
    "print('Length of Crosby Tokens:', len(crosbytokens))\n",
    "\n",
    "total = len(messitokens)+len(lebrontokens)+len(trouttokens)+len(mahomestokens)+len(crosbytokens)\n",
    "print('Length of All Tokens:', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2169e81a-4620-462e-b359-e6fb1aaef20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling list of stopwords\n",
    "\n",
    "punctuation_stop_list = string.punctuation\n",
    "\n",
    "# originial list of stop words\n",
    "stoplist = stopwords.words(\"english\")\n",
    "\n",
    "# adding more stop words\n",
    "stoplist.extend([\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\", \n",
    "    \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \n",
    "    \"can't\", \"cannot\", \"could\", \"couldn't\", \n",
    "    \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\", \"during\", \n",
    "    \"each\", \n",
    "    \"few\", \"for\", \"from\", \"further\", \n",
    "    \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \n",
    "    \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \n",
    "    \"let's\", \n",
    "    \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \n",
    "    \"no\", \"nor\", \"not\", \n",
    "    \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "    \".\", \",\", \"'s\", \"--\", \"n't\", \"ha\", \"wa\"\n",
    "])\n",
    "\n",
    "# adding punctuation\n",
    "stoplist.extend(punctuation_stop_list)\n",
    "\n",
    "# adding more punctuation\n",
    "stoplist.extend([\"''\", '``'])\n",
    "\n",
    "stopwords = set(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43245ae7-3a33-45d9-a829-ec67f3d7425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (1.26.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: jinja2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: setuptools in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kevinkoh/Desktop/textproc/venv/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa274da-cea4-4711-ac4a-329f30c71216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "index=['Messi', 'Lebron', 'Trout', 'Crosby', 'Mahomes']\n",
    "\n",
    "# Your texts\n",
    "texts = [\" \".join(messitokens), \" \".join(lebrontokens), \" \".join(trouttokens), \" \".join(crosbytokens), \" \".join(mahomestokens)]\n",
    "\n",
    "def extract_adjectives(text):\n",
    "    # Split the text into chunks if it's too long\n",
    "    chunk_size = 1000000  # spaCy's default max length\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "    adjectives = []\n",
    "    for chunk in chunks:\n",
    "        doc = nlp(chunk)\n",
    "        adjectives.extend([token.text for token in doc if token.pos_ == \"ADJ\"])\n",
    "    \n",
    "    return adjectives\n",
    "\n",
    "# Extract adjectives from each text\n",
    "adjectives_per_text = [extract_adjectives(text) for text in texts]\n",
    "\n",
    "# If you want to see the adjectives for each athlete\n",
    "for x in range(len(index)):\n",
    "    print(index[x])\n",
    "    print(adjectives_per_text[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4f6d7-6332-4180-8314-ecb4e2811c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "messi_lemmatized = [lemmatizer.lemmatize(w) for w in messitokens]\n",
    "lebron_lemmatized = [lemmatizer.lemmatize(w) for w in lebrontokens]\n",
    "mahomes_lemmatized = [lemmatizer.lemmatize(w) for w in mahomestokens]\n",
    "crosby_lemmatized = [lemmatizer.lemmatize(w) for w in crosbytokens]\n",
    "trout_lemmatized = [lemmatizer.lemmatize(w) for w in trouttokens]\n",
    "\n",
    "messi_nostopwords = [w for w in messi_lemmatized if w not in stopwords]\n",
    "lebron_nostopwords = [w for w in lebron_lemmatized if w not in stopwords]\n",
    "mahomes_nostopwords = [w for w in mahomes_lemmatized if w not in stopwords]\n",
    "crosby_nostopwords = [w for w in crosby_lemmatized if w not in stopwords]\n",
    "trout_nostopwords = [w for w in trout_lemmatized if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ed7fb-68b5-49e1-b5f1-5db9bcae9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building word clouds w stopwords\n",
    "\n",
    "print('Messi Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(messi_lemmatized)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "messitext = \" \".join(messi_lemmatized)                                                                                                                                                                \n",
    "wordcloud = WordCloud(max_font_size=40).generate(messitext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/messi.png')\n",
    "plt.show()\n",
    "\n",
    "print('Lebron Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(lebron_lemmatized)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "lebrontext = \" \".join(lebron_lemmatized)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(lebrontext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/lebron.png')\n",
    "plt.show()\n",
    "\n",
    "print('Mahomes Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(mahomes_lemmatized)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "mahomestext = \" \".join(mahomes_lemmatized)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(mahomestext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/mahomes.png')\n",
    "plt.show()\n",
    "\n",
    "print('Crosby Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(crosby_lemmatized)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "crosbytext = \" \".join(crosby_lemmatized)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(crosbytext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/crosby.png')\n",
    "plt.show()\n",
    "\n",
    "print('Trout Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(trout_lemmatized)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "trouttext = \" \".join(trout_lemmatized)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(trouttext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/trout.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c500a8-640e-4fe4-9ea9-fcfe702b1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building word clouds w/o stopwords\n",
    "\n",
    "print('Messi Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(messi_nostopwords)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "messitext = \" \".join(messi_nostopwords)                                                                                                                                                                \n",
    "wordcloud = WordCloud(max_font_size=40).generate(messitext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/messi.png')\n",
    "plt.show()\n",
    "\n",
    "print('Lebron Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(lebron_nostopwords)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "lebrontext = \" \".join(lebron_nostopwords)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(lebrontext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/lebron.png')\n",
    "plt.show()\n",
    "\n",
    "print('Mahomes Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(mahomes_nostopwords)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "mahomestext = \" \".join(mahomes_nostopwords)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(mahomestext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/mahomes.png')\n",
    "plt.show()\n",
    "\n",
    "print('Crosby Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(crosby_nostopwords)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "crosbytext = \" \".join(crosby_nostopwords)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(crosbytext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/crosby.png')\n",
    "plt.show()\n",
    "\n",
    "print('Trout Word Cloud')\n",
    "\n",
    "fdist = nltk.FreqDist(trout_nostopwords)\n",
    "print(\"\\nFrequency Distribution\")\n",
    "print(fdist.most_common(20))\n",
    "print(\" \")\n",
    "\n",
    "trouttext = \" \".join(trout_nostopwords)                                                                                                                                                             \n",
    "wordcloud = WordCloud(max_font_size=40).generate(trouttext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig('img/trout.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268baf30-a9b5-4dcd-a8f3-bf7ff767591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_common_bigrams(tokenlist):\n",
    "    \n",
    "    bigrams = nltk.ngrams(tokenlist, 2)\n",
    "    bigramlist = list(bigrams)\n",
    "    \n",
    "    # print out most frequent bigrams\n",
    "    bigramfreq = nltk.FreqDist(bigramlist)\n",
    "    top10bigrams = bigramfreq.most_common(10)\n",
    "    top50bigrams = bigramfreq.most_common(50)\n",
    "\n",
    "    # part a)\n",
    "    print('** Most frequent bigrams **')\n",
    "    for x in top10bigrams:\n",
    "        print(x[0][0], x[0][1])\n",
    "\n",
    "    \n",
    "    # part b)\n",
    "    print('\\n** Most frequent bigrams with no stop words **')\n",
    "    for x in top50bigrams:\n",
    "        if x[0][0].lower() not in stoplist and x[0][1].lower() not in stoplist:\n",
    "            print(x[0][0], x[0][1])\n",
    "\n",
    "\n",
    "print('Messi Bigrams')\n",
    "print_common_bigrams(messi_lemmatized)\n",
    "print('\\nLebron Bigrams')\n",
    "print_common_bigrams(lebron_lemmatized)\n",
    "print('\\nTrout Bigrams')\n",
    "print_common_bigrams(trout_lemmatized)\n",
    "print('\\nCrosby Bigrams')\n",
    "print_common_bigrams(crosby_lemmatized)\n",
    "print('\\nMahomes Bigrams')\n",
    "print_common_bigrams(mahomes_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f535f-121b-455a-895f-4d35f4386515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statement\n",
    "from nltk.collocations import *\n",
    "\n",
    "# Create the object you need to get collocations.\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "def print_collocations(tokenlist):\n",
    "\n",
    "    finder = BigramCollocationFinder.from_words(tokenlist)\n",
    "    finder.apply_freq_filter(2)\n",
    "    print('** Common Collocations **')\n",
    "    for c in finder.nbest(bigram_measures.pmi, 10):\n",
    "        print(\" \".join(c))\n",
    "\n",
    "print('Messi Collocations')\n",
    "print_collocations(messi_lemmatized)\n",
    "print('\\nLebron Collocations')\n",
    "print_collocations(lebron_lemmatized)\n",
    "print('\\nTrout Collocations')\n",
    "print_collocations(trout_lemmatized)\n",
    "print('\\nCrosby Collocations')\n",
    "print_collocations(crosby_lemmatized)\n",
    "print('\\nMahomes Collocations')\n",
    "print_collocations(mahomes_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933e215-63d6-4d95-ab17-f6c6e20e03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "index=['Messi', 'Lebron', 'Trout', 'Crosby', 'Mahomes']\n",
    "alltexts = [\" \".join(messi_nostopwords), \" \".join(lebron_nostopwords), \" \".join(trout_nostopwords), \" \".join(crosby_nostopwords), \" \".join(mahomes_nostopwords),]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(input='content', stop_words='english')\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(alltexts)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=index, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df = tfidf_df.stack().reset_index()\n",
    "tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document', 'level_1': 'term'})\n",
    "top_terms = tfidf_df.sort_values(by=['document', 'tfidf'], ascending=[True, False]).groupby(['document']).head(10)\n",
    "\n",
    "top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f2371-4a31-40aa-976a-cf8850ead250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will create the heatmap \n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# adding a little randomness to break ties in term ranking\n",
    "top_tfidf_plusRand = top_terms.copy()\n",
    "top_tfidf_plusRand['tfidf'] = top_tfidf_plusRand['tfidf'] + np.random.rand(top_terms.shape[0])*0.0001\n",
    "\n",
    "# base for all visualizations, with rank calculation\n",
    "base = alt.Chart(top_tfidf_plusRand).encode(\n",
    "    x = 'rank:O',\n",
    "    y = 'document:N'\n",
    ").transform_window(\n",
    "    rank = \"rank()\",\n",
    "    sort = [alt.SortField(\"tfidf\", order=\"descending\")],\n",
    "    groupby = [\"document\"],\n",
    ")\n",
    "\n",
    "# heatmap specification\n",
    "heatmap = base.mark_rect().encode(\n",
    "    color = 'tfidf:Q'\n",
    ")\n",
    "\n",
    "# text labels, white for darker heatmap colors\n",
    "text = base.mark_text(baseline='middle').encode(\n",
    "    text = 'term:N',\n",
    "    color = alt.condition(alt.datum.tfidf >= 0.23, alt.value('white'), alt.value('black'))\n",
    ")\n",
    "\n",
    "# display the three superimposed visualizations\n",
    "(heatmap + text).properties(width = 600)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
